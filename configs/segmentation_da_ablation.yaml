# Experiment configuration file.
#
# There are two special blocks. The 'seml' block is required for every experiment.
# It has to contain the following values:
# db_collection: Name of the MongoDB collection to save the experiment information to
# executable:    Name of the Python script containing the experiment
# Additionally, it can contain a `conda_environment` entry which specifies which conda environment will be activated
# before execution of the executable.
#
# The special 'slurm' block contains the slurm parameters. This block and all values are optional. Possible values are:
# name:                 Job name used by Slurm and file name of Slurm output. Default: Collection name
# output_dir:           Directory to store the Slurm log files in. Default: Current directory
# experiments_per_job:  Number of parallel experiments to run in each Slurm job.
#                       Note that only experiments from the same batch share a job. Default: 1
# sbatch_options:       dictionary that contains custom values that will be passed to `sbatch`, specifying e.g.
#                       the memory and number of GPUs to be allocated (prepended dashes are not required). See
#                       https://slurm.schedmd.com/sbatch.html for all possible options.
#
# Parameters under 'fixed' will be used for all the experiments.
#
# Under 'grid' you can define parameters that should be sampled from a regular grid. Options are:
#   - choice:     List the different values you want to evaluate under 'choices' as in the example below.
#   - range:      Specify the min, max, and step. Parameter values will be generated using np.arange(min, max, step).
#   - uniform:    Specify the min, max, and num. Parameter values will be generated using
#                 np.linspace(min, max, num, endpoint=True)
#   - loguniform: Specify min, max, and num. Parameter values will be uniformly generated in log space (base 10).
#
# Under 'random' you can specify parameters for which you want to try several random values. Specify the number
# of samples per parameter with the 'samples' value as in the examples below.
# Specify the the seed under the 'random' dict or directly for the desired parameter(s).
# Supported parameter types are:
#   - choice:      Randomly samples <samples> entries (with replacement) from the list in parameter['options']
#   - uniform:     Uniformly samples between 'min' and 'max' as specified in the parameter dict.
#   - loguniform:  Uniformly samples in log space between 'min' and 'max' as specified in the parameter dict.
#   - randint:     Randomly samples integers between 'min' (included) and 'max' (excluded).
#
# The configuration file can be nested (as the example below) so that we can run different parameter sets
# e.g. for different datasets or models.
# We take the cartesian product of all `grid` parameters on a path and sample all random parameters on the path.
# The number of random parameters sampled will be max{n_samples} of all n_samples on the path. This is done because
# we need the same number of samples from all random parameters in a configuration.
#
# More specific settings (i.e., further down the hierarchy) always overwrite more general ones.


seml:
  name: segmentation_da_ablation
  project_root_dir: /nfs/homedirs/mirlas/ssda
  executable: /nfs/homedirs/mirlas/ssda/segmentation_da.py
  conda_environment: ssda
  output_dir: /nfs/homedirs/mirlas/ssda/slurm-output/

slurm:
  experiments_per_job: 1
  sbatch_options:
    gres: gpu:1       # num GPUs
    mem: 16G          # memory
    cpus-per-task: 4  # num cores
    time: 10-00:00    # max time, D-HH:MM

###### BEGIN PARAMETER CONFIGURATION ######

fixed:
  # data params
  source_dir: /nfs/students/mirlas/data/
  target_dir: /nfs/students/mirlas/data/
  snapshots_dir: /nfs/students/mirlas/snapshots/seg_da/
  log_dir: /nfs/students/mirlas/logdir/seg_da/
  pred_dir: /nfs/students/mirlas/predictions/seg_da/
  source_train_images: gta5_images_train.txt
  source_train_labels: gta5_labels_train.txt
  target_train_images: cityscapes_images_train.txt
  target_train_labels: cityscapes_labels_train.txt
  source_val_images: gta5_images_val.txt
  source_val_labels: gta5_labels_val.txt
  target_val_images: cityscapes_images_val.txt
  target_val_labels: cityscapes_labels_val.txt  
  train_batch_size: 2
  val_batch_size: 2
  image_height: 400
  image_width: 800
  num_workers: 4
  # network params
  backbone: resnet50
  classification_head: fcn
  pretrained_backbone: True
  segmentation_loss: ce         #  [ce, focal]
  gamma: 0.0                    #  only used if segmentation_loss is focal
  # optim params
  learning_rate: 0.00025
  momentum: 0.9
  weight_decay: 0.0005
  lrs_power: 0.9
  learning_rate_da: 0.0001
  betas_da: [0.9, 0.99]
  # training loop params
  max_iter: 200000
  epoch_to_resume: 0
  batches_to_eval_train: 5
  batches_to_visualize: 5
  points_to_sample: 1000
  save_step: 10
  display_step: 100
  seed: 42

grid:
  da_injection_point:     #  [output, feature]
    type: choice
    options:
      - output
      - feature
  lambda_da:
    type: choice
    options:
      - 0.001
      - 0.01
      - 0.1